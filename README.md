# How Sememic Components Can Benefit Link Prediction for Lexico-Semantic Knowledge Graphs?
[![Paper](https://img.shields.io/badge/Paper-EMNLP%202025-blue)](https://arxiv.org/abs/XXX)

Data and code for the paper "How Sememic Components Can Benefit Link Prediction for Lexico-Semantic Knowledge Graphs?" in EMNLP 2025 Main.

## ğŸ“Š Data
We provide the SememeDef dataset for Sememe Prediction (SP), along with two Chinese datasets, HN7 and CWN5, for Link Prediction (LP), aiming to alleviate the scarcity of both SP and Chinese LP resources.

### 1. SememeDef (Sememe Prediction Dataset)
#### General description
Our SememeDef dataset contains a substantial amount of word senses with sememe annotations for both English and Chinese (70,645 English samples and 43,163 Chinese ones, covering 2,042 and 1,762 sememes, respectively). Among them, the English data is integrated from [SememeBabel](https://github.com/thunlp/MSGI), while the Chinese data is obtained based on Word Sense Alignment (WSA) results between Hownet and CCD.

To respect intellectual property rights and better meet the needs of computational applications, we have referred to the approach of [MiCLS](https://github.com/COOLPKU/MorBERT), condensed the relevant semantic space to some extent, and made extensive revisions and optimizations to the word sense definitions.

Here, we have open-sourced a subset of the Chinese part of this resource, which includes 20,492 word senses for 15,000 words (the vocabulary is consistent with the [MiCLS](https://github.com/COOLPKU/MorBERT) resource previously open-sourced by [COOLPKU](https://github.com/COOLPKU/COOL). As a result, the experimental results for Chinese in the paper may be slightly affected during replication.

These resources are uploaded to `data/SememeDef/`. The file structure is as follows:
- `data/SememeDef/`: data files
  - `en.txt`: Data for English
  - `zh.txt`: Data for Chinese


#### Data Format
Each sample includes a word sense definition, corresponding sememes, and the main sememe. The columns are separated by a space. The format and example of the data are shown in the table:
|Column|Description|Example|
|:----|:----|:----|
|**0**|Word Sense Definition|a person whose job is teaching|
|**1**|Sememes|human\|äºº,occupation\|èŒä½,education\|æ•™è‚²,teach\|æ•™|
|**2**|MainSememe|human\|äºº|



### 2. HN7 (Chinese LP Dataset)
#### Overview
HN7 is built on HowNet, focusing on lexico-semantic relations with fine-grained sense distinctions. It follows the WN18RR protocol to avoid test leakage and provides unified sense definitions generated by GPT-4o.

#### Dataset Details
| Item                | Value                  |
|---------------------|------------------------|
| Synsets Count       | 10,939                 |
| Relation Types      | 7 (antonymy, hypernymy, hyponymy, holonymy, meronymy, material, product) |
| Total Triples       | 25,672                 |
| Split Ratio (Train:Val:Test) | 8:1:1 |
| Train Triples       | 13,624                 |
| Validation Triples  | 1,702                  |
| Test Triples        | 1,703                  |

#### Data Format
Triples are stored as [head_synset, relation, tail_synset], with additional sense definition files. File format [to be filled by user, e.g., TSV/JSON]:
```
# Triples file (hn7_triples.tsv)
head_synset_id    relation    tail_synset_id
synset_001        antonymy    synset_002
synset_003        hypernymy   synset_004

# Definitions file (hn7_definitions.json)
{
  "synset_001": {
    "words": ["å¹¼", "å°‘"],
    "definition": "å¹´çºªå°çš„ï¼›æœªé•¿æˆ"
  },
  "synset_002": {
    "words": ["å¹´è¿ˆ", "è€"],
    "definition": "å¹´å²å¤§çš„ï¼›å¹´é•¿çš„"
  }
}
```

### 3. CWN5 (Chinese LP Dataset)
#### Overview
CWN5 is derived from Chinese WordNet (CWN), converted to simplified Chinese. It retains core lexico-semantic relations and follows the same evaluation protocol as HN7.

#### Dataset Details
| Item                | Value                  |
|---------------------|------------------------|
| Synsets Count       | 3,149                  |
| Relation Types      | 5 (antonymy, hypernymy, hyponymy, holonymy, meronymy) |
| Total Triples       | 5,395                  |
| Split Ratio (Train:Val:Test) | 8:1:1 |
| Train Triples       | 2,600                  |
| Validation Triples  | 324                    |
| Test Triples        | 325                    |

#### Data Format
Consistent with HN7 for unified processing. File format [to be filled by user, e.g., TSV/JSON]:
```
# Triples file (cwn5_triples.tsv)
head_synset_id    relation    tail_synset_id
synset_c001       antonymy    synset_c002
synset_c003       hypernymy   synset_c004

# Definitions file (cwn5_definitions.json)
{
  "synset_c001": {
    "words": ["å¦‡å¥³", "å¥³äºº"],
    "definition": "æˆå¹´å¥³å­"
  },
  "synset_c002": {
    "words": ["ä¸", "ç”·äºº"],
    "definition": "æˆå¹´ç”·å­"
  }
}
```

## ğŸ› ï¸ Code
### Installation
We recommend using `conda` to manage the environment.

1. Create and activate the environment:
```bash
conda create -n sememelp python==3.10
conda activate sememelp
```

2. Clone the repository and install dependencies:
```bash
git clone https://github.com/[your-username]/SememeLP.git
cd SememeLP
pip3 install -e .
# Install additional dependencies (to be filled by user, e.g., torch, transformers)
# pip3 install torch==2.1.0 transformers==4.35.2 ...
```

3. Install optional dependencies (if needed):
```bash
# For GPU acceleration (example)
conda install -c pytorch -c nvidia faiss-gpu=1.8.0
# For FlashAttention (optional)
pip3 install flash-attn --no-build-isolation
```

### Quick Start
#### 1. Data Preparation
- Download the datasets from [to be filled by user, e.g., Google Drive/Weiyun] and extract them to the `data/` directory.
- The directory structure should be:
```
SememeLP/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sememedef/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â””â”€â”€ val/
â”‚   â”œâ”€â”€ hn7/
â”‚   â”‚   â”œâ”€â”€ train_triples.tsv
â”‚   â”‚   â”œâ”€â”€ val_triples.tsv
â”‚   â”‚   â”œâ”€â”€ test_triples.tsv
â”‚   â”‚   â””â”€â”€ definitions.json
â”‚   â””â”€â”€ cwn5/
â”‚       â”œâ”€â”€ train_triples.tsv
â”‚       â”œâ”€â”€ val_triples.tsv
â”‚       â”œâ”€â”€ test_triples.tsv
â”‚       â””â”€â”€ definitions.json
â””â”€â”€ src/
```

#### 2. Training
We provide training scripts for two core stages: (1) Sememe Encoder Fine-tuning (SP task), (2) SememeLP Training (LP task).

##### Stage 1: Fine-tune Sememe Encoder
```bash
cd scripts/train
bash train_sememe_encoder.sh \
    --train_data ../data/sememedef/train \
    --val_data ../data/sememedef/val \
    --model_name [base-model-name, e.g., chinese-bert-base-wwm-ext] \
    --output_dir ../checkpoints/sememe_encoder \
    # Additional hyperparameters (to be filled by user)
```

##### Stage 2: Train SememeLP
```bash
cd scripts/train
bash train_sememelp.sh \
    --dataset [hn7/cwn5/wn18rr] \
    --data_dir ../data/[hn7/cwn5] \
    --sememe_encoder_path ../checkpoints/sememe_encoder/best_model \
    --output_dir ../checkpoints/sememelp \
    # Additional hyperparameters (to be filled by user, e.g., batch_size, epochs)
```

#### 3. Inference
Use the trained SememeLP model for link prediction inference:
```bash
cd scripts/inference
python run_inference.py \
    --model_path ../checkpoints/sememelp/best_model \
    --dataset [hn7/cwn5] \
    --test_data ../data/[hn7/cwn5]/test_triples.tsv \
    --output_path ../results/inference_output.json \
    # Additional parameters (to be filled by user)
```

#### 4. Evaluation
Evaluate the model performance using MRR, Hits@1/3/10 metrics:
```bash
cd scripts/evaluation
python run_eval.py \
    --pred_path ../results/inference_output.json \
    --ground_truth_path ../data/[hn7/cwn5]/test_triples.tsv \
    --metric [mrr/hits@1/hits@3/hits@10] \
    --output_path ../results/eval_results.txt \
    # Additional parameters (to be filled by user)
```

## ğŸ“ˆ Results
SememeLP achieves SOTA performance on English and Chinese LP datasets:
| Dataset  | MRR   | Hits@1 | Hits@3 | Hits@10 |
|----------|-------|--------|--------|---------|
| WN18RR   | 75.1% | 67.6%  | 79.8%  | 88.5%   |
| HN7      | 80.5% | 74.6%  | 84.0%  | 91.8%   |
| CWN5     | 77.1% | 69.2%  | 82.5%  | 90.6%   |

For detailed results and analysis, refer to our paper.

## ğŸ¤ Acknowledgements
This work is based on the following open-source projects:
- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [PyTorch](https://pytorch.org/)
- [HowNet](https://www.keenage.com/)
- [Chinese WordNet](https://lope.linguistics.ntu.edu.tw/cwn/)

We sincerely appreciate their contributions to the community.

## ğŸ“š Citation
If you find this work useful, please cite our paper:
```bibtex
@inproceedings{sememelp2025,
  title={How Sememic Components Can Benefit Link Prediction for Lexico-Semantic Knowledge Graphs?},
  author={[Author Names]},
  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2025}
}
```

## ğŸ“„ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

è¦ä¸è¦æˆ‘å¸®ä½ è¡¥å……**æ•°æ®é›†ä¸‹è½½é“¾æ¥æ¨¡æ¿**æˆ–**ä»£ç å ä½ç¬¦çš„å…·ä½“ç¤ºä¾‹**ï¼Ÿæ¯”å¦‚ç»†åŒ–è®­ç»ƒè„šæœ¬çš„å‚æ•°è¯´æ˜ï¼Œæˆ–è¡¥å……æ•°æ®é›†çš„æ ·ä¾‹æ–‡ä»¶å†…å®¹ã€‚
